{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0ab4eb1",
   "metadata": {},
   "source": [
    "### RecSystems trained on past data\n",
    "Recommender systems learn from what happened before. So if historical data reflects unfair patterns — for example women getting turned away from tech jobs — then the RecSys will keep turning them away. In other words, old stereotypes just get reinforced in new forms. For example, a job-matching RecSys might keep showing stereotypical roles, recommending HR positions to women and engineering to men. An adversarial tweak can help remove or reduce gender signals so recommendations broaden out. . \n",
    "\n",
    "### Popularity dominates the training data\n",
    "We keep seeing the same blockbuster movies or chart-topping hits. Because “popular” items dominate the training data, and collaborative-filtering algorithms happily feed that back to us. Niche or minority interests get buried under the majority of mainstream content. A streaming service may over-recommend action movies to male users because more men historically watched them. A post-processing re-ranking step can inject more varied genres to ensure that female viewers aren’t locked into a narrow slice of the catalogue. \n",
    "\n",
    "### Hidden Demographic Signals\n",
    "Even if model gets stripped out of explicit sensitive attributes (like gender or race), there’s enough implicit info to reintroduce bias, such as browsing habits, past ratings, even time of day. Simply omitting a field called “gender” doesn’t make the model blind to gender-like patterns . If an online retailer’s RecSys notices that certain ZIP codes (tied to income or race) has tendencies to buy high-discounted or \"on sale\" items , it might end up targeting only those neighborhoods with discounts or low-quality cheap items, effectively segmenting customers unethically. Data-augmentation upfront can smooth out those distortions. \n",
    "\n",
    "### Fairness tools\n",
    "\n",
    "1. **Fix the training data in pre-processing**\n",
    "\n",
    "Before even train the model, tweak the data so it’s more balanced.\n",
    "\n",
    "**What to do**: Add extra (real or synthetic) ratings for under-represented groups or items.\n",
    "\n",
    "**Why it helps**:  model won’t just learn “everyone loves the same 5 songs” because it was spread in enough variety up front.\n",
    "\n",
    "2. **Teach the model to neutralize adversaries in learning stage**\n",
    "\n",
    "While the model is learning, actively punish it for using sensitive clues.\n",
    "\n",
    "**What to do**: Slot in an adversary (a mini-model) whose job is to sniff out gender/race info in the hidden layers—and penalize the main model if any leaks through.\n",
    "\n",
    "**Why it helps**: The recommender learns to make predictions without relying on those unfair shortcuts.\n",
    "\n",
    "3. **Tweak the output in post-processing**\n",
    "\n",
    "After recommender spits out a ranked list, remix it for fairness.\n",
    "\n",
    "**What to do**: Boost items or user groups that got too little exposure, can be shuffled or re-ranked so everyone gets a fair shot.\n",
    "\n",
    "**Why it helps**: tweak the recommendations for fairness after they’re made \n",
    "\n",
    "### Bottom line:\n",
    "\n",
    "**Pre-processing** is about fixing data before training.\n",
    "\n",
    "**In-processing** is about penalizing bias during training.\n",
    "\n",
    "**Post-processing** is about adjusting recommendations after the fact.\n",
    "\n",
    "Reference paper: “Consumer-side fairness in recommender systems: a systematic survey of methods and evaluation.” https://link.springer.com/article/10.1007/s10462-023-10663-5?fromPaywallRec=false"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
