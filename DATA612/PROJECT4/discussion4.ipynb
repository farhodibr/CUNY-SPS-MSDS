{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5a4fd1",
   "metadata": {},
   "source": [
    "## DATA 612\n",
    "### Research Discussion Assignment 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bf1ef",
   "metadata": {},
   "source": [
    "Yes, I'm partially agree with the statement \"Today, recommendation engines are perhaps the biggest threat to societal cohesion on the internet\" &nbsp;statement from \"Up Next: A Better Recommendation System\" article by Renee Diresta https://www.wired.com/story/creating-ethical-recommendation-engines/.&nbsp; &nbsp;I also came across this article \"Recommendation Systems and Extremism: What Do We Know?\" by Joe Whittaker https://gnet-research.org/2022/08/17/recommendation-systems-and-extremism-what-do-we-know/ which also points that the main problem is in recommender systems' algorithms or black boxes.\n",
    "\n",
    "Honestly, I think we’re pointing fingers at the wrong place—users drive these engines more than the companies do. Algorithms don’t have an agenda; they’re just responding to what we click, watch, and share. If you binge conspiracy videos, the system will dutifully queue up more of them. If you mostly watch cute animal clips, you’ll keep seeing puppies and kittens.\n",
    "\n",
    "Both articles authors make the same point in different ways: recommendation systems mirror our own interests. Diresta’s Pinterest anecdote about how a single click on anti-Islamic memes completely warped her feed shows how fast the “more like this” loop can spiral. Jigsaw’s Project Redirect is a neat hack—when YouTube spots someone hunting extremist content, it punts them to de-radicalization clips or fact checks—but it still feels reactive, like slapping a band-aid on a problem we keep inflicting on ourselves.\n",
    "\n",
    "Whittaker’s literature survey digs into the research: about two-thirds of studies find some amplification of extreme videos in platforms like YouTube, but many also show that self-selection and offline social networks play an even bigger role in radicalization. A few recent papers even suggest YouTube quietly nudges you back toward the mainstream when you start going off the rails. The truth is, “radicalization by algorithm” is a lot messier than the headlines imply, and most studies rely on black-box snapshots of YouTube’s API rather than the full personalized experience.\n",
    "\n",
    "So yeah, we could and should tweak the tech—sliders for “deep dive vs. discovery,” hiding star-ratings until enough people vote, injecting random or moderate picks into your feed—but at the end of the day it’s our own click habits that build the echo chambers. We need to take more responsibility for our browsing: consciously mix up what we watch, try out new channels, use filters or “tune my feed” controls, and even lean into fact-check redirects when we stumble onto sketchy stuff. Algorithms are happy to feed us what we want—maybe the real change starts with us choosing something different."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
